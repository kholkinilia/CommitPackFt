{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Refact evaluation on Python HumanEvalFix with pass@1 metric"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f14a36e5b3e87627"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's evaluate [Refact](https://huggingface.co/smallcloudai/Refact-1_6B-fim) model that used [CommitPackFt](https://huggingface.co/datasets/bigcode/commitpackft) dataset in its training!\n",
    "\n",
    "Let's do the evaluation following way:\n",
    "1. Generate the fixed version of the functions\n",
    "2. Create checker scripts that would import the generated function from the fixed versions\n",
    "3. Run the scripts and count, how many succeeded\n",
    "4. Finally, look at the results! "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "421f26e93f79ef06"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Some technicalities\n",
    "\n",
    "First, some technicalities. Just importing some stuff..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed63952b77fa72e2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:38:20.563505Z",
     "start_time": "2023-12-29T02:38:19.845360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# some imports\n",
    "\n",
    "import datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from subprocess import TimeoutExpired\n",
    "from constants import HUMAN_EVAL_PACK_DATASET, HUMAN_EVAL_PACK_LANG, PYTHON, REFACT_MODEL\n",
    "from utils import get_refact_prompt, parse_refact_response, get_checke_code, get_id\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's download a Python subset of [HumanEvalPack](https://huggingface.co/datasets/bigcode/humanevalpack) dataset that is used for HumanEvalFix... "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d96b0592c8fa0f4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task_id', 'prompt', 'declaration', 'canonical_solution', 'buggy_solution', 'bug_type', 'failure_symptoms', 'entry_point', 'import', 'test_setup', 'test', 'example_test', 'signature', 'docstring', 'instruction'],\n",
      "    num_rows: 164\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hep_python = datasets.load_dataset(HUMAN_EVAL_PACK_DATASET, HUMAN_EVAL_PACK_LANG[PYTHON])['test']\n",
    "print(hep_python)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:38:23.921049Z",
     "start_time": "2023-12-29T02:38:20.560968Z"
    }
   },
   "id": "ea6167c493b1a1ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to get a model..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c08586ad45e20d5"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "checkpoint = REFACT_MODEL\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "refact_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "refact_model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:38:27.386632Z",
     "start_time": "2023-12-29T02:38:23.896877Z"
    }
   },
   "id": "3d48ecebc9e5185"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating\n",
    "\n",
    "Alright, now that we have everything we need, let me describe the process of generation:\n",
    "\n",
    "As was suggested, I use the **experimental** chat prompt provided by Refactor model team:\n",
    "\n",
    "```\n",
    "<empty_output>SYSTEM {system} \n",
    "<empty_output>USER {user} \n",
    "<empty_output>ASSISTANT\n",
    "```\n",
    "\n",
    "It has a system prompt and a user prompt.\n",
    "\n",
    "As for system prompt, I haven't found the recommended one by Refactor model team. So I created my own:\n",
    "\n",
    ">You are a programming assistant that generates bug fixes for Python functions. A user will provide you with a function and will provide some testcases in the following format: '\\<function definition\\>[whitespace]\\<testcase definition\\>[whitespace]\\<instruction to fix bugs\\>'. You need to generate code of the correct implementation of the function. \n",
    "\n",
    "It seems like the result will depend a lot on the system prompt. So, we'll try another one later to see, what will happen.\n",
    "\n",
    "As for user prompt, I took the format from the paper:\n",
    "```\n",
    "<buggy function>\n",
    "\n",
    "<testcases>\n",
    "\n",
    "Fix bugs in <buggy function name>.\n",
    "```\n",
    "\n",
    "I generate an output from the model and get the first function from it. Sometimes the model generates several versions and is cut off by the token limit. This results in not working python files that contain a valid function. So, I take the first function from the output."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b416ebf5b8dc0400"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We put generated code in `gen{id}.py` files in `EVAL_DATA_DIR` that I define above. Later we will put checker scripts `check{id}.py` there as well. Just for the sake of convenience..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fd62a052e06c029"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "EVAL_DATA_DIR = \"eval_data\"\n",
    "if not os.path.exists(EVAL_DATA_DIR):\n",
    "    os.mkdir(EVAL_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:38:27.421377Z",
     "start_time": "2023-12-29T02:38:27.387525Z"
    }
   },
   "id": "6e99dc8d78ba11b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, let's write a function that will generate the fixed code using a model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f2d3dea4059a823"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def generate_fix(item: dict, model, tokenizer, output_dir: str):\n",
    "    \"\"\"\n",
    "    A function that generates fixed versions of functions and saves it output_dir by name \"gen{id}.py\", where id is an int value from item['task_id']\n",
    "    :param tokenizer: A tokenizer to tokenize a prompt with\n",
    "    :param model: A model to generate with\n",
    "    :param item: an item from HumanEvalPack dataset to generate a fix for\n",
    "    :param output_dir: a directory, where the generated files are stored\n",
    "    \"\"\"\n",
    "    fix_prompt = get_refact_prompt(item)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    inputs = tokenizer.encode(fix_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_length=max(1024, 2 * inputs.shape[1]), temperature=0.2)\n",
    "\n",
    "    code = parse_refact_response(tokenizer.decode(outputs[0]))\n",
    "    filename = f\"gen{get_id(item)}.py\"\n",
    "    with open(os.path.join(output_dir, filename), \"w\") as file:\n",
    "        file.write(code)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:38:27.437675Z",
     "start_time": "2023-12-29T02:38:27.407881Z"
    }
   },
   "id": "8e47887656baa204"
  },
  {
   "cell_type": "markdown",
   "source": [
    "No we go to Colab and run the following to get generations... (I kept `eval_data` directory with generations in repo, so that you don't have to do it)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22d71021d4e39c6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for item in hep_python:\n",
    "    generate_fix(item, refact_model, refact_tokenizer, EVAL_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdced70220509062"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alright, we have our proposed fixes. Let's write checkers for them. Checkers are not complicated they are just:\n",
    "```\n",
    "from gen{id} import {function_name}\n",
    "\n",
    "{testcases}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58e0e052e5f41e68"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def create_checker(item: dict, output_dir: str):\n",
    "    \"\"\"\n",
    "    A function that creates a checker script for an item given a directory where to put the checker script. The file is saved by name \"check{id}.py\", where id is an int value from item['task_id']\n",
    "    :param item: An item from HumanEvalPack dataset to generate a script for\n",
    "    :param output_dir: A directory to save the script to. Should contain gen{id}.py for a checker to work properly\n",
    "    \"\"\"\n",
    "    code = get_checke_code(item)\n",
    "\n",
    "    filename = f\"check{get_id(item)}.py\"\n",
    "    with open(os.path.join(output_dir, filename), \"w\") as file:\n",
    "        file.write(code)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:38:29.835538Z",
     "start_time": "2023-12-29T02:38:29.807580Z"
    }
   },
   "id": "a11d2d66bb33c0d4"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "for item in hep_python:\n",
    "    create_checker(item, EVAL_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:38:31.230425Z",
     "start_time": "2023-12-29T02:38:31.181009Z"
    }
   },
   "id": "8e262d27f98b4241"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can look at the results. Let's run all checkers and store the result of their completion. \n",
    "\n",
    "A result is successful if a checker finished with error code 0 in two seconds. A two-second threshold is introduced to cut off infinite loops. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba74a174e28680f6"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_results(eval_dir):\n",
    "    \"\"\"\n",
    "    Returns a numpy array of shape 164 (the number of python items in HumanEvalFix task). An array consists of 0s and 1s. A value at index i is 1 iff the checker with id i has successfully passed.\n",
    "    :param eval_dir: A directory with checkers. Should contain all 164 checkers with filenames in a format \"check{id}.py\"\n",
    "    :return: An array with results\n",
    "    \"\"\"\n",
    "    result = np.zeros(len(hep_python)).astype(np.bool_)\n",
    "    for id in tqdm(range(len(hep_python))):\n",
    "        checker_filename = f\"check{id}.py\"\n",
    "        checker_path = os.path.join(eval_dir, checker_filename)\n",
    "        \n",
    "        try:\n",
    "            cur_result = subprocess.run(['python', checker_path], capture_output=True, text=True, timeout=2)\n",
    "            \n",
    "            result[id] = cur_result.returncode == 0\n",
    "        except TimeoutExpired:\n",
    "            print(f\"Checker#{id} timed out\")\n",
    "        \n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:38:49.429640Z",
     "start_time": "2023-12-29T02:38:49.410871Z"
    }
   },
   "id": "c4c81388f19a93d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 152/164 [00:01<00:00, 85.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checker#156 timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:06<00:00, 27.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checker#160 timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.putenv(\"TOKENIZERS_PARALLELISM\",\"false\") # a line to remove transformers warnings complaining about creating subprocesses\n",
    "\n",
    "results = get_results(EVAL_DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:38:56.661651Z",
     "start_time": "2023-12-29T02:38:50.577824Z"
    }
   },
   "id": "e6db924269afb375"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(threshold turned out to be useful)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1525789da5f86e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at the results!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6eaccd4212768536"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 score: 0.07317073170731707\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pass@1 score: {np.count_nonzero(results) / results.size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:39:13.146703Z",
     "start_time": "2023-12-29T02:39:13.116340Z"
    }
   },
   "id": "b9712ff011c9b4fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, 7% pass@1... Not that impressive. Although the Refact guys claimed that they have 18% on the same task. I assume it depends significantly on the system prompt.\n",
    "\n",
    "So why not trying to change it! (but first we also print out the scores of each bug type)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae0d05b6d177f076"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "item_bug_types = np.array([item['bug_type'] for item in hep_python])\n",
    "bug_types, bug_type_counts = np.unique(item_bug_types, return_counts=True) \n",
    "type_to_id = {val: i for i, val in enumerate(bug_types)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:39:14.847618Z",
     "start_time": "2023-12-29T02:39:14.814465Z"
    }
   },
   "id": "afaf3d796c304528"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "results_by_types = np.zeros(len(bug_types))\n",
    "for i, result in enumerate(results):\n",
    "    results_by_types[type_to_id[item_bug_types[i]]] += int(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:39:15.972567Z",
     "start_time": "2023-12-29T02:39:15.941719Z"
    }
   },
   "id": "b23c4a3aa7323f65"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result across bug types:\n",
      "excess logic   : 0.0967741935483871\n",
      "function misuse: 0.0\n",
      "missing logic  : 0.030303030303030304\n",
      "operator misuse: 0.12\n",
      "value misuse   : 0.06818181818181818\n",
      "variable misuse: 0.08695652173913043\n"
     ]
    }
   ],
   "source": [
    "result_percents = results_by_types/bug_type_counts\n",
    "print(\"Result across bug types:\")\n",
    "for i, tpe in enumerate(bug_types):\n",
    "    print(f\"{tpe}{' '*(max([len(t) for t in bug_types]) - len(tpe))}: {result_percents[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:39:16.729436Z",
     "start_time": "2023-12-29T02:39:16.702976Z"
    }
   },
   "id": "3770fd8fcc0e443f"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5d868d58fa0b349e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "It handled \"operator misuse\" best and it didn't fix any of the function misuse bugs. They models proposed in the paper have operator misuse as one of the best as well, but the worse performance they have on excess logic, which is not the case for Refactor model with the system prompt that I wrote.\n",
    "![picture](../resources/results_across_types.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d98f874fc306ac4"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items across bug types:\n",
      "excess logic   : 31\n",
      "function misuse: 8\n",
      "missing logic  : 33\n",
      "operator misuse: 25\n",
      "value misuse   : 44\n",
      "variable misuse: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of items across bug types:\")\n",
    "for i, tpe in enumerate(bug_types):\n",
    "    print(f\"{tpe}{' '*(max([len(t) for t in bug_types]) - len(tpe))}: {bug_type_counts[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:39:20.521184Z",
     "start_time": "2023-12-29T02:39:20.489678Z"
    }
   },
   "id": "973383e8996d4dcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### New prompt!\n",
    "\n",
    "Alright, let's create a better system prompt and achieve better results! (hopefully)\n",
    "\n",
    "The second version of the prompt specifies rigorously the input and output formats. I also ask the model to generate a comment on how to fix the bug before writing a function:\n",
    "> You are a programming assistant that generates bug fixes for Python functions. A user will provide you with a bugged function and will provide some testcases in the following format: '<bugged_function>[whitespace]<check_function>[whitespace]<fix_instruction>', where \"bugged_function\" is the function you need to corred, \"check_function\" is the function with testcases that the correct solution must pass, \"fix_instruction\" is the instruction to fix a bug.  \n",
    "> \n",
    "> Your response should be in the following format: '<imports>[whitespace]<description_comment>[whitespace]<correct implementation>', where \"imports\" are required imports, \"description_comment\" is a python comment that describes a bug and a way to fix it with natural language, \"correct_implementation\" is the correct implementation of a bugged function.\n",
    "> \n",
    "> Before generating the correct function, you should describe a bug and how to fix it in a natural language Python comment.\n",
    "\n",
    "Let's see how it will perform this time! (I kept `eval_data_new_prompt` directory with generation in repo, so that you don't have to generate it)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8aeabdbff7bd44c7"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "EVAL_DATA_NEW_DIR = \"eval_data_new_prompt\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:39:22.049959Z",
     "start_time": "2023-12-29T02:39:22.018679Z"
    }
   },
   "id": "1ba9536af144d2e0"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "for item in hep_python:\n",
    "    create_checker(item, EVAL_DATA_NEW_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:39:22.628089Z",
     "start_time": "2023-12-29T02:39:22.571687Z"
    }
   },
   "id": "a426cd1697fd6ffb"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 155/164 [00:02<00:00, 81.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checker#156 timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:06<00:00, 26.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checker#160 timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.putenv(\"TOKENIZERS_PARALLELISM\",\"false\") # a line to remove transformers warnings complaining about creating subprocesses\n",
    "\n",
    "new_results = get_results(EVAL_DATA_NEW_DIR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:39:29.187549Z",
     "start_time": "2023-12-29T02:39:22.980528Z"
    }
   },
   "id": "9afdf8a4b4d62d82"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1 score: 0.024390243902439025\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pass@1 score: {np.count_nonzero(new_results) / new_results.size}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:42:23.825673Z",
     "start_time": "2023-12-29T02:42:23.789445Z"
    }
   },
   "id": "2aa41c3973b1eed5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cool! With a new prompt the results are more than two times worse!\n",
    "\n",
    "At least it proves that the results depend on the prompt a lot...\n",
    "\n",
    "Here are the results with the new prompt across different bug types..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d2646161f29533c"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "new_results_by_types = np.zeros(len(bug_types))\n",
    "for i, result in enumerate(new_results):\n",
    "    new_results_by_types[type_to_id[item_bug_types[i]]] += int(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:42:24.431930Z",
     "start_time": "2023-12-29T02:42:24.412793Z"
    }
   },
   "id": "315d2d78b6952965"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excess logic   : 0.03225806451612903\n",
      "function misuse: 0.0\n",
      "missing logic  : 0.0\n",
      "operator misuse: 0.08\n",
      "value misuse   : 0.0\n",
      "variable misuse: 0.043478260869565216\n"
     ]
    }
   ],
   "source": [
    "new_result_percents = new_results_by_types/bug_type_counts\n",
    "for i, tpe in enumerate(bug_types):\n",
    "    print(f\"{tpe}{' '*(max([len(t) for t in bug_types]) - len(tpe))}: {new_result_percents[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:42:25.007085Z",
     "start_time": "2023-12-29T02:42:24.985560Z"
    }
   },
   "id": "43c6465051f41cef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Looking and generations\n",
    "\n",
    "Let's look at generations and try to understand, what went wrong!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ced37e207abe0f"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def get_generations(required_bug_type: str, required_result: bool, check_results: list, gen_dir: str, return_indexes: bool = False):\n",
    "    \"\"\"\n",
    "    A function that returns a list of strings: generations for the required bug type and required result.\n",
    "    :param required_bug_type: A required bug type\n",
    "    :param required_result: A required result\n",
    "    :param check_results: A list of results\n",
    "    :param gen_dir: A directory, where generations lie \n",
    "    :param return_indexes: If true, ids are returned as well\n",
    "    :return: A list of matching generations\n",
    "    \"\"\"\n",
    "    gens = []\n",
    "    gen_ids = []\n",
    "    for i in range(len(hep_python)):\n",
    "        if check_results[i] == required_result and required_bug_type == item_bug_types[i]:\n",
    "            with open(os.path.join(gen_dir, f\"gen{i}.py\"), \"r\") as file:\n",
    "                gens.append(file.read())\n",
    "            gen_ids.append(i)\n",
    "    \n",
    "    if return_indexes:\n",
    "        return gens, gen_ids\n",
    "    return gens\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:42:28.554901Z",
     "start_time": "2023-12-29T02:42:28.517465Z"
    }
   },
   "id": "c6451ca722bca6e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### What went wrong with \"function misuse\"\n",
    "\n",
    "First let's look at some generations for \"function misuse\" class:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40daff21435f2a05"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "fm_gens, fm_ids = get_generations(\"function misuse\", False, results, EVAL_DATA_DIR, return_indexes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:42:29.686746Z",
     "start_time": "2023-12-29T02:42:29.662984Z"
    }
   },
   "id": "7a33cd916985b62a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def flip_case(string: str) -> str:\n",
      "    return string.lower()\n",
      "====================================================================================================\n",
      "from typing import List\n",
      "\n",
      "\n",
      "\n",
      "def filter_by_prefix(strings: List[str], prefix: str) -> List[str]:\n",
      "    return [x for x in strings if x.endswith(prefix)]\n",
      "====================================================================================================\n",
      "def digitSum(s):\n",
      "    if s == \"\": return 0\n",
      "    return sum(ord(char) if char.islower() else 0 for char in s)\n"
     ]
    }
   ],
   "source": [
    "print((\"\\n\"+\"=\"*100+\"\\n\").join(fm_gens[:3]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:42:30.278652Z",
     "start_time": "2023-12-29T02:42:30.243380Z"
    }
   },
   "id": "9eca15b4f0e8ffb5"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def get_buggy_function(item: dict) -> str:\n",
    "    \"\"\"\n",
    "    Returns a buggy function that passes testcases for an item from Python HumanEvalPack dataset\n",
    "    :param item: An item to get a function from\n",
    "    :return: A string representing a function definition\n",
    "    \"\"\"\n",
    "    return item['declaration'] + item['buggy_solution']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:42:35.257110Z",
     "start_time": "2023-12-29T02:42:35.225333Z"
    }
   },
   "id": "db7c50f56d0a1c5a"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def flip_case(string: str) -> str:\n",
      "    return string.lower()\n",
      "====================================================================================================\n",
      "from typing import List\n",
      "\n",
      "\n",
      "def filter_by_prefix(strings: List[str], prefix: str) -> List[str]:\n",
      "    return [x for x in strings if x.endswith(prefix)]\n",
      "====================================================================================================\n",
      "def digitSum(s):\n",
      "    if s == \"\": return 0\n",
      "    return sum(ord(char) if char.islower() else 0 for char in s)\n"
     ]
    }
   ],
   "source": [
    "fm_buggy = [get_buggy_function(hep_python[id]).strip() for id in fm_ids]\n",
    "\n",
    "print((\"\\n\"+\"=\"*100+\"\\n\").join(fm_buggy[:3]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:42:35.375718Z",
     "start_time": "2023-12-29T02:42:35.350288Z"
    }
   },
   "id": "36de30dc1bdf641a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seems like it kept the solutions the same. How many functions did it change?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de12fd1a0e9d6d54"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def is_same(impl1: str, impl2: str) -> bool:\n",
    "    \"\"\"\n",
    "    A function that determines, whether two implementations are the same character-wise. It pays attention only to meaningful characters.\n",
    "    \n",
    "    :param impl1: One implementation\n",
    "    :param impl2: Other implementation\n",
    "    :returns: True is both implementations are the same\n",
    "    \"\"\"\n",
    "    def remove_comments(impl: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes trivial comments from the implementation\n",
    "        :param impl: An implementation to remove the comments from\n",
    "        :returns: An implementation cleared from trivial comments\n",
    "        \"\"\" \n",
    "        lines = impl.split(\"\\n\")\n",
    "        return \"\\n\".join([line for line in lines if not line.startswith(\"#\")])\n",
    "    return remove_comments(impl1).split() == remove_comments(impl2).split()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:48:01.692585Z",
     "start_time": "2023-12-29T02:48:01.651042Z"
    }
   },
   "id": "d18419bccddc2df9"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of function that were left the same across bug types (only failed fixes are considered):\n",
      "excess logic   : 22/28\n",
      "function misuse: 7/8\n",
      "missing logic  : 23/32\n",
      "operator misuse: 15/22\n",
      "value misuse   : 30/41\n",
      "variable misuse: 15/21\n",
      "TOTAL SAME: 112/152\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of function that were left the same across bug types (only failed fixes are considered):\")\n",
    "\n",
    "total_same = 0\n",
    "total_wrong = 0\n",
    "for tpe in bug_types:\n",
    "    tpe_gens, tpe_ids = get_generations(tpe, False, results, EVAL_DATA_DIR, return_indexes=True)\n",
    "    equal = [is_same(tpe_gens[k], get_buggy_function(hep_python[tpe_ids[k]])) for k in range(len(tpe_gens))]\n",
    "    total_same += np.count_nonzero(equal)\n",
    "    total_wrong += len(tpe_gens)\n",
    "    print(f\"{tpe}{' '*(max([len(t) for t in bug_types]) - len(tpe))}: {np.count_nonzero(equal)}/{len(tpe_gens)}\")\n",
    "print(f\"TOTAL SAME: {total_same}/{total_wrong}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:48:30.082270Z",
     "start_time": "2023-12-29T02:48:30.031705Z"
    }
   },
   "id": "9867980f9b0d6d3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "So the main reason of a failure is that the model was keeping the functions the same."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8e5d6bbe54c5a44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What went wrong with a new prompt?\n",
    "\n",
    "First, we need to pay attention that the new prompt changed the behaviour of the model.\n",
    "\n",
    "It started generating docstrings (some are correct, according to my eyes) for a lot of functions:\n",
    "\n",
    "```\n",
    "def encode_shift(s: str):\n",
    "    \"\"\"\n",
    "    returns encoded string by shifting every character by 5 in the alphabet.\n",
    "    \"\"\"\n",
    "    return \"\".join([chr(((ord(ch) + 5 - ord(\"a\")) % 26) + ord(\"a\")) for ch in s])\n",
    "```\n",
    "\n",
    "But it didn't help with changing the code and functions mostly are kept the same as well (checked using eyes).\n",
    "\n",
    "Sometimes it even did, what I asked it to do (generate a comment on how to fix a bug), but in a different order. \n",
    "In the following example it didn't change the code, but outlined the correct change:\n",
    "\n",
    "```\n",
    "def fizz_buzz(n: int):\n",
    "    ns = []\n",
    "    for i in range(n):\n",
    "        if i % 11 == 0 and i % 13 == 0:\n",
    "            ns.append(i)\n",
    "    s = ''.join(list(map(str, ns)))\n",
    "    ans = 0\n",
    "    for c in s:\n",
    "        ans += (c == '7')\n",
    "    return ans\n",
    "\n",
    "# The bug in fizz_buzz is that it does not correctly handle numbers that are multiples of both 11 and 13.\n",
    "# To fix this, we need to add an additional condition to the if statement that checks if the current number is a multiple of both 11 and 13.\n",
    "# We can do this by adding an else statement that appends the current number to the list if it is not a multiple of either 11 or 13.\n",
    "```\n",
    "\n",
    "In the last example with the original prompt, the model failed as well. But it \"sees\" the bug. Meaning that most likely with some smart prompt corrections results may be way better.\n",
    "\n",
    "And, finally, just for the sake of interest, some functions that were correct with the original prompt and that are incorrect with the current one:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea9cfd1a448d2570"
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================== ORIGINAL PROMPT ========================================\n",
      "from typing import List\n",
      "\n",
      "\n",
      "\n",
      "def all_prefixes(string: str) -> List[str]:\n",
      "    result = []\n",
      "\n",
      "    for i in range(len(string)):\n",
      "        result.append(string[:i+1])\n",
      "    return result\n",
      "======================================== NEW PROMPT ========================================\n",
      "from typing import List\n",
      "\n",
      "\n",
      "\n",
      "def all_prefixes(string: str) -> List[str]:\n",
      "    \"\"\"\n",
      "    Returns a list of all possible prefixes of a given string.\n",
      "\n",
      "    Example:\n",
      "    >>> all_prefixes('')\n",
      "    []\n",
      "    >>> all_prefixes('asdfgh')\n",
      "    ['a', 'as', 'asd', 'asdf', 'asdfg', 'asdfgh']\n",
      "    >>> all_prefixes('WWW')\n",
      "    ['W', 'WW', 'WWW']\n",
      "    \"\"\"\n",
      "    result = []\n",
      "\n",
      "    for i in range(len(string)-1):\n",
      "        result.append(string[:i+1])\n",
      "    return result\n",
      "====================================================================================================\n",
      "======================================== ORIGINAL PROMPT ========================================\n",
      "def digits(n):\n",
      "    product = 1\n",
      "    odd_count = 0\n",
      "    for digit in str(n):\n",
      "        int_digit = int(digit)\n",
      "        if int_digit%2 == 1:\n",
      "            product*= int_digit\n",
      "            odd_count+=1\n",
      "    if odd_count ==0:\n",
      "        return 0\n",
      "    else:\n",
      "        return product\n",
      "======================================== NEW PROMPT ========================================\n",
      "def digits(n):\n",
      "    product = 1\n",
      "    odd_count = 0\n",
      "    for digit in str(n):\n",
      "        int_digit = int(digit)\n",
      "        if int_digit%2 == 1:\n",
      "            product*= product*int_digit\n",
      "            odd_count+=1\n",
      "    if odd_count ==0:\n",
      "        return 0\n",
      "    else:\n",
      "        return product\n",
      "====================================================================================================\n",
      "======================================== ORIGINAL PROMPT ========================================\n",
      "from typing import List\n",
      "\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    for idx, elem in enumerate(numbers):\n",
      "        for idx2, elem2 in enumerate(numbers):\n",
      "            if idx!= idx2:\n",
      "                distance = abs(elem - elem2)\n",
      "                if distance < threshold:\n",
      "                    return True\n",
      "\n",
      "    return False\n",
      "======================================== NEW PROMPT ========================================\n",
      "from typing import List\n",
      "\n",
      "\n",
      "\n",
      "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a list of numbers contains elements that are close to each other.\n",
      "\n",
      "    Parameters:\n",
      "    numbers (List[float]): The list of numbers to check.\n",
      "    threshold (float): The maximum difference between elements to consider them close.\n",
      "\n",
      "    Returns:\n",
      "    bool: True if the list contains elements that are close to each other, False otherwise.\n",
      "    \"\"\"\n",
      "    for idx, elem in enumerate(numbers):\n",
      "        for idx2, elem2 in enumerate(numbers):\n",
      "            if idx!= idx2:\n",
      "                distance = elem - elem2\n",
      "                if distance < threshold:\n",
      "                    return True\n",
      "\n",
      "    return False\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "fake_results = np.logical_and(results, np.logical_not(new_results))\n",
    "\n",
    "N = 3\n",
    "count = 0\n",
    "\n",
    "for tpe in bug_types:\n",
    "    tpe_orig = get_generations(tpe, True, fake_results, EVAL_DATA_DIR)\n",
    "    tpe_new = get_generations(tpe, True, fake_results, EVAL_DATA_NEW_DIR)\n",
    "    for i in range(len(tpe_orig)):\n",
    "        count += 1\n",
    "        print(f\"{'='*40} ORIGINAL PROMPT {'='*40}\")\n",
    "        print(tpe_orig[i])\n",
    "        print(f\"{'='*40} NEW PROMPT {'='*40}\")\n",
    "        print(tpe_new[i])\n",
    "        print(\"=\"*100)\n",
    "        if count == N:\n",
    "            break\n",
    "    if count == N:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T22:40:41.199371Z",
     "start_time": "2023-12-28T22:40:41.147738Z"
    }
   },
   "id": "4e1a9c3c63ca5211"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
